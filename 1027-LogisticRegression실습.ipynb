{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"1LH6JJmChFoB6sJvKAz1-DGDpcrr8GBBu","authorship_tag":"ABX9TyNVh9Y6qtS1nqNfZjLDu6Md"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"5OXQJqVWfRnW"},"outputs":[],"source":["# Titanic Data Set을 이용해서 이진분류(Logistic Regression)을\n","# 구현해 보아요!\n","\n","# 필요한 module을 import\n","import numpy as np\n","import pandas as pd\n","from sklearn import linear_model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Flatten, Dense\n","from tensorflow.keras.optimizers import Adam\n","\n","# Raw Data Loading\n","titanic = pd.read_csv('/content/drive/MyDrive/[빅데이터 과정 공유폴더]/data/titanic/train.csv')\n","display(titanic.head(3), titanic.shape)  # (891, 12)\n","\n","# 사용하지 않는 컬럼부터 제거!\n","titanic.drop(['PassengerId', 'Name', 'Ticket', 'Fare', 'Cabin'],\n","             axis=1, inplace=True)\n","display(titanic.head(3), titanic.shape)  # (891, 12)"]},{"cell_type":"code","source":["# 데이터 전처리를 해 보아요!\n","gender_dict = {'male': 0,\n","               'female': 1 }\n","titanic['Sex'] = titanic['Sex'].map(gender_dict)\n","display(titanic.head(3), titanic.shape)  # (891, 12)\n"],"metadata":{"id":"8QkEprgjlFRA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 가족처리\n","titanic['Family'] = titanic['SibSp'] + titanic['Parch']\n","titanic.drop(['SibSp', 'Parch'], axis=1, inplace=True)\n","display(titanic.head(3), titanic.shape)  # (891, 12)"],"metadata":{"id":"s-2ebGeX3wpg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["titanic.info()"],"metadata":{"id":"Z5_toA4f4U9P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Embarked 컬럼에 결측치가 2개가 있어요!\n","# 결측치를 찾아서 'Q'로 대체할꺼예요!\n","titanic['Embarked'] = titanic['Embarked'].fillna('Q')\n","\n","# Embarked 컬럼의 영문자를 숫자로 바꿔줄꺼예요!\n","embarked_mapping = {'S': 0,\n","                    'C': 1,\n","                    'Q': 2}\n","titanic['Embarked'] = titanic['Embarked'].map(embarked_mapping)\n","display(titanic.head(3), titanic.shape)  # (891, 12)"],"metadata":{"id":"v3VgIKlR4zjP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 나이중에 NaN이 있어요!\n","# 처리해야 해요.. 그런데 NaN의 비중이 20%되요!\n","# 이런경우에는 삭제하면 당연히 좋지 않아요!\n","# NaN값을 적절하게 대체해서 사용할꺼예요.. 어떻게 대체하느냐에 따라\n","# 머신러닝의 결과가 많이 달라질 수 있어요. 그래서 조심해야 해요!\n","# 다른 머신러닝 기법을 이용해서 이 NaN이 어떤값이 되면 좋을지를 추측하는 방법\n","# 다른 방법은 전통적인 통계치를 이용하는 방법이 있고 일반적으로 많이 사용\n","# 평균값을 사용해요!\n","\n","# 가장 간단한 방법은 나이의 평균값을 구해서 NaN을 채우는 방법이예요!\n","# 여자는 여자의 나이 평균을 구해서 채우고 남자는 남자의 나이 평균을 구해서 채워요!\n","titanic['Age'] = titanic['Age'].fillna(titanic['Age'].mean())"],"metadata":{"id":"0PSUTsAw7yz8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 나이는 어떻게 처리하면 좋을까요?\n","# 그냥 실제값을 사용하는게 좋은가요? => 좋지 않을꺼 같아요!\n","# Binning처리를 해요! (구간을 이용해서 어떤 구간에 포함되는지를 명시)\n","# 8살 미만이면 0\n","# 8살부터 20살까지면 1 이런식으로 처리해볼꺼예요!\n","\n","# 8살 미만이면 0으로 나이를 대체할꺼예요!\n","titanic.loc[titanic['Age'] < 8, 'Age'] = 0\n","titanic.loc[(titanic['Age'] >= 8) & (titanic['Age'] < 20), 'Age'] = 1\n","titanic.loc[(titanic['Age'] >= 20) & (titanic['Age'] < 65), 'Age'] = 2\n","titanic.loc[titanic['Age'] >= 65, 'Age'] = 3\n","display(titanic.head(3), titanic.shape)  # (891, 6)"],"metadata":{"id":"PYvhqPdZ5wm3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터 전처리가 끝나요!\n","\n","# 전체 Training data set을 준비해 보아요!\n","x_data = titanic.drop('Survived', axis=1, inplace=False).values\n","t_data = titanic['Survived'].values.reshape(-1,1)\n","\n","# training data와 test data로 분리해야 해요!\n","# 그런 다음 training data를 다시 training과 validation용으로 다시 나누어야 해요!\n","# 그런데 이런 validation 데이터를 나눠서 중간평가를 하는건 Keras가 대신 해줘요!\n","# 따라서 특별한 경우가 아니면 validation data는 keras를 이용해서 처리하면 되요!"],"metadata":{"id":"YognUQgR7ljN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Model을 만들면 되요!\n","\n","keras_model = Sequential()\n","\n","keras_model.add(Flatten(input_shape=(5,)))\n","keras_model.add(Dense(units=1,\n","                      activation='sigmoid'))\n","\n","keras_model.compile(optimizer=Adam(learning_rate=1e-2),\n","                    loss='binary_crossentropy',\n","                    metrics=['accuracy'])\n","\n","keras_model.fit(x_data,\n","                t_data,\n","                epochs=300,\n","                verbose=1,\n","                validation_split=0.2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y74crKbw-xDE","executionInfo":{"status":"ok","timestamp":1698384578384,"user_tz":-540,"elapsed":27035,"user":{"displayName":"문성훈","userId":"17340983977521415755"}},"outputId":"a2633ef5-fb55-42c6-f273-7773bd6d7d88"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/300\n","23/23 [==============================] - 1s 8ms/step - loss: 1.3896 - accuracy: 0.3904 - val_loss: 1.0709 - val_accuracy: 0.3575\n","Epoch 2/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.8696 - accuracy: 0.5520 - val_loss: 0.7010 - val_accuracy: 0.7654\n","Epoch 3/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.6653 - accuracy: 0.6770 - val_loss: 0.5961 - val_accuracy: 0.7486\n","Epoch 4/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.6017 - accuracy: 0.6798 - val_loss: 0.5524 - val_accuracy: 0.7318\n","Epoch 5/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.5672 - accuracy: 0.7233 - val_loss: 0.5239 - val_accuracy: 0.7877\n","Epoch 6/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.5437 - accuracy: 0.7823 - val_loss: 0.5062 - val_accuracy: 0.8045\n","Epoch 7/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.5279 - accuracy: 0.7809 - val_loss: 0.4920 - val_accuracy: 0.7989\n","Epoch 8/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.5164 - accuracy: 0.7837 - val_loss: 0.4828 - val_accuracy: 0.7989\n","Epoch 9/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.5071 - accuracy: 0.7837 - val_loss: 0.4753 - val_accuracy: 0.7989\n","Epoch 10/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.5008 - accuracy: 0.7851 - val_loss: 0.4680 - val_accuracy: 0.8101\n","Epoch 11/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4960 - accuracy: 0.7907 - val_loss: 0.4627 - val_accuracy: 0.8101\n","Epoch 12/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4916 - accuracy: 0.7893 - val_loss: 0.4584 - val_accuracy: 0.8101\n","Epoch 13/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4884 - accuracy: 0.7879 - val_loss: 0.4570 - val_accuracy: 0.8101\n","Epoch 14/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4853 - accuracy: 0.7851 - val_loss: 0.4523 - val_accuracy: 0.8101\n","Epoch 15/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4831 - accuracy: 0.7865 - val_loss: 0.4495 - val_accuracy: 0.8101\n","Epoch 16/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4815 - accuracy: 0.7865 - val_loss: 0.4468 - val_accuracy: 0.8101\n","Epoch 17/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4796 - accuracy: 0.7921 - val_loss: 0.4453 - val_accuracy: 0.8101\n","Epoch 18/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4784 - accuracy: 0.7907 - val_loss: 0.4438 - val_accuracy: 0.8101\n","Epoch 19/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4773 - accuracy: 0.7879 - val_loss: 0.4427 - val_accuracy: 0.8101\n","Epoch 20/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4767 - accuracy: 0.7865 - val_loss: 0.4419 - val_accuracy: 0.8101\n","Epoch 21/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4751 - accuracy: 0.7921 - val_loss: 0.4406 - val_accuracy: 0.8156\n","Epoch 22/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4738 - accuracy: 0.7921 - val_loss: 0.4395 - val_accuracy: 0.8101\n","Epoch 23/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4737 - accuracy: 0.7921 - val_loss: 0.4384 - val_accuracy: 0.8101\n","Epoch 24/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4727 - accuracy: 0.7893 - val_loss: 0.4394 - val_accuracy: 0.8101\n","Epoch 25/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4721 - accuracy: 0.7893 - val_loss: 0.4373 - val_accuracy: 0.8101\n","Epoch 26/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4718 - accuracy: 0.7907 - val_loss: 0.4364 - val_accuracy: 0.8101\n","Epoch 27/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4707 - accuracy: 0.7921 - val_loss: 0.4358 - val_accuracy: 0.8101\n","Epoch 28/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4708 - accuracy: 0.7893 - val_loss: 0.4365 - val_accuracy: 0.8101\n","Epoch 29/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4702 - accuracy: 0.7921 - val_loss: 0.4338 - val_accuracy: 0.8101\n","Epoch 30/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4696 - accuracy: 0.7992 - val_loss: 0.4340 - val_accuracy: 0.8156\n","Epoch 31/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4698 - accuracy: 0.7963 - val_loss: 0.4330 - val_accuracy: 0.8101\n","Epoch 32/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4692 - accuracy: 0.7949 - val_loss: 0.4325 - val_accuracy: 0.8101\n","Epoch 33/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4684 - accuracy: 0.7921 - val_loss: 0.4323 - val_accuracy: 0.8101\n","Epoch 34/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4685 - accuracy: 0.7907 - val_loss: 0.4316 - val_accuracy: 0.8101\n","Epoch 35/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4686 - accuracy: 0.7879 - val_loss: 0.4315 - val_accuracy: 0.8101\n","Epoch 36/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4680 - accuracy: 0.7978 - val_loss: 0.4297 - val_accuracy: 0.8212\n","Epoch 37/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4668 - accuracy: 0.7879 - val_loss: 0.4302 - val_accuracy: 0.8101\n","Epoch 38/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4669 - accuracy: 0.7893 - val_loss: 0.4298 - val_accuracy: 0.8101\n","Epoch 39/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4679 - accuracy: 0.7949 - val_loss: 0.4279 - val_accuracy: 0.8101\n","Epoch 40/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4674 - accuracy: 0.7893 - val_loss: 0.4306 - val_accuracy: 0.8101\n","Epoch 41/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4663 - accuracy: 0.7949 - val_loss: 0.4280 - val_accuracy: 0.8212\n","Epoch 42/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4656 - accuracy: 0.8006 - val_loss: 0.4276 - val_accuracy: 0.8101\n","Epoch 43/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4663 - accuracy: 0.7921 - val_loss: 0.4274 - val_accuracy: 0.8101\n","Epoch 44/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4658 - accuracy: 0.7992 - val_loss: 0.4259 - val_accuracy: 0.8212\n","Epoch 45/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4660 - accuracy: 0.7879 - val_loss: 0.4260 - val_accuracy: 0.8212\n","Epoch 46/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4659 - accuracy: 0.7949 - val_loss: 0.4246 - val_accuracy: 0.8212\n","Epoch 47/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4657 - accuracy: 0.8034 - val_loss: 0.4249 - val_accuracy: 0.8212\n","Epoch 48/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4640 - accuracy: 0.7963 - val_loss: 0.4280 - val_accuracy: 0.8101\n","Epoch 49/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4640 - accuracy: 0.7949 - val_loss: 0.4237 - val_accuracy: 0.8212\n","Epoch 50/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4649 - accuracy: 0.7935 - val_loss: 0.4234 - val_accuracy: 0.8101\n","Epoch 51/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4635 - accuracy: 0.7992 - val_loss: 0.4221 - val_accuracy: 0.8156\n","Epoch 52/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4637 - accuracy: 0.7921 - val_loss: 0.4236 - val_accuracy: 0.8101\n","Epoch 53/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4625 - accuracy: 0.8020 - val_loss: 0.4222 - val_accuracy: 0.8212\n","Epoch 54/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4651 - accuracy: 0.8020 - val_loss: 0.4220 - val_accuracy: 0.8212\n","Epoch 55/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4646 - accuracy: 0.8034 - val_loss: 0.4212 - val_accuracy: 0.8268\n","Epoch 56/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4620 - accuracy: 0.8006 - val_loss: 0.4205 - val_accuracy: 0.8156\n","Epoch 57/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4621 - accuracy: 0.7978 - val_loss: 0.4211 - val_accuracy: 0.8156\n","Epoch 58/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4628 - accuracy: 0.8006 - val_loss: 0.4186 - val_accuracy: 0.8156\n","Epoch 59/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4613 - accuracy: 0.8006 - val_loss: 0.4196 - val_accuracy: 0.8156\n","Epoch 60/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4646 - accuracy: 0.7978 - val_loss: 0.4194 - val_accuracy: 0.8156\n","Epoch 61/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4618 - accuracy: 0.7949 - val_loss: 0.4182 - val_accuracy: 0.8156\n","Epoch 62/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4616 - accuracy: 0.8006 - val_loss: 0.4178 - val_accuracy: 0.8268\n","Epoch 63/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4610 - accuracy: 0.7992 - val_loss: 0.4172 - val_accuracy: 0.8156\n","Epoch 64/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4620 - accuracy: 0.7907 - val_loss: 0.4195 - val_accuracy: 0.8101\n","Epoch 65/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4617 - accuracy: 0.7992 - val_loss: 0.4157 - val_accuracy: 0.8212\n","Epoch 66/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4610 - accuracy: 0.7978 - val_loss: 0.4167 - val_accuracy: 0.8268\n","Epoch 67/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4610 - accuracy: 0.7992 - val_loss: 0.4154 - val_accuracy: 0.8212\n","Epoch 68/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4605 - accuracy: 0.7992 - val_loss: 0.4152 - val_accuracy: 0.8156\n","Epoch 69/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4602 - accuracy: 0.7992 - val_loss: 0.4142 - val_accuracy: 0.8156\n","Epoch 70/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4603 - accuracy: 0.7992 - val_loss: 0.4134 - val_accuracy: 0.8212\n","Epoch 71/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4610 - accuracy: 0.7992 - val_loss: 0.4144 - val_accuracy: 0.8156\n","Epoch 72/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4595 - accuracy: 0.8034 - val_loss: 0.4132 - val_accuracy: 0.8324\n","Epoch 73/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4604 - accuracy: 0.7992 - val_loss: 0.4127 - val_accuracy: 0.8156\n","Epoch 74/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4597 - accuracy: 0.7978 - val_loss: 0.4120 - val_accuracy: 0.8156\n","Epoch 75/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4588 - accuracy: 0.7992 - val_loss: 0.4127 - val_accuracy: 0.8156\n","Epoch 76/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4588 - accuracy: 0.7992 - val_loss: 0.4112 - val_accuracy: 0.8156\n","Epoch 77/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4593 - accuracy: 0.7992 - val_loss: 0.4114 - val_accuracy: 0.8212\n","Epoch 78/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4587 - accuracy: 0.8006 - val_loss: 0.4126 - val_accuracy: 0.8156\n","Epoch 79/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4590 - accuracy: 0.7992 - val_loss: 0.4118 - val_accuracy: 0.8156\n","Epoch 80/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4598 - accuracy: 0.7949 - val_loss: 0.4100 - val_accuracy: 0.8212\n","Epoch 81/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4591 - accuracy: 0.7992 - val_loss: 0.4099 - val_accuracy: 0.8212\n","Epoch 82/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4589 - accuracy: 0.7992 - val_loss: 0.4097 - val_accuracy: 0.8212\n","Epoch 83/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4582 - accuracy: 0.7978 - val_loss: 0.4109 - val_accuracy: 0.8212\n","Epoch 84/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4579 - accuracy: 0.7978 - val_loss: 0.4103 - val_accuracy: 0.8212\n","Epoch 85/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4586 - accuracy: 0.7992 - val_loss: 0.4081 - val_accuracy: 0.8212\n","Epoch 86/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4583 - accuracy: 0.7978 - val_loss: 0.4093 - val_accuracy: 0.8212\n","Epoch 87/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4584 - accuracy: 0.7978 - val_loss: 0.4102 - val_accuracy: 0.8212\n","Epoch 88/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4602 - accuracy: 0.7978 - val_loss: 0.4094 - val_accuracy: 0.8268\n","Epoch 89/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4579 - accuracy: 0.8006 - val_loss: 0.4082 - val_accuracy: 0.8268\n","Epoch 90/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4579 - accuracy: 0.7978 - val_loss: 0.4077 - val_accuracy: 0.8324\n","Epoch 91/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4594 - accuracy: 0.7992 - val_loss: 0.4073 - val_accuracy: 0.8212\n","Epoch 92/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4588 - accuracy: 0.7978 - val_loss: 0.4062 - val_accuracy: 0.8268\n","Epoch 93/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4579 - accuracy: 0.8006 - val_loss: 0.4052 - val_accuracy: 0.8212\n","Epoch 94/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4573 - accuracy: 0.7978 - val_loss: 0.4054 - val_accuracy: 0.8212\n","Epoch 95/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4569 - accuracy: 0.7992 - val_loss: 0.4069 - val_accuracy: 0.8156\n","Epoch 96/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4566 - accuracy: 0.7978 - val_loss: 0.4048 - val_accuracy: 0.8212\n","Epoch 97/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4574 - accuracy: 0.7978 - val_loss: 0.4039 - val_accuracy: 0.8212\n","Epoch 98/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4566 - accuracy: 0.7978 - val_loss: 0.4045 - val_accuracy: 0.8212\n","Epoch 99/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4574 - accuracy: 0.7978 - val_loss: 0.4055 - val_accuracy: 0.8212\n","Epoch 100/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4588 - accuracy: 0.7992 - val_loss: 0.4039 - val_accuracy: 0.8268\n","Epoch 101/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4586 - accuracy: 0.7978 - val_loss: 0.4048 - val_accuracy: 0.8212\n","Epoch 102/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4562 - accuracy: 0.7992 - val_loss: 0.4035 - val_accuracy: 0.8212\n","Epoch 103/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4569 - accuracy: 0.7978 - val_loss: 0.4032 - val_accuracy: 0.8268\n","Epoch 104/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4566 - accuracy: 0.7978 - val_loss: 0.4042 - val_accuracy: 0.8212\n","Epoch 105/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4578 - accuracy: 0.7978 - val_loss: 0.4036 - val_accuracy: 0.8268\n","Epoch 106/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4572 - accuracy: 0.7992 - val_loss: 0.4051 - val_accuracy: 0.8212\n","Epoch 107/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4566 - accuracy: 0.7992 - val_loss: 0.4044 - val_accuracy: 0.8268\n","Epoch 108/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4578 - accuracy: 0.7992 - val_loss: 0.4050 - val_accuracy: 0.8212\n","Epoch 109/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4616 - accuracy: 0.7963 - val_loss: 0.4022 - val_accuracy: 0.8324\n","Epoch 110/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4606 - accuracy: 0.7963 - val_loss: 0.4044 - val_accuracy: 0.8212\n","Epoch 111/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4562 - accuracy: 0.7978 - val_loss: 0.4010 - val_accuracy: 0.8324\n","Epoch 112/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4560 - accuracy: 0.7978 - val_loss: 0.4020 - val_accuracy: 0.8212\n","Epoch 113/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4562 - accuracy: 0.7992 - val_loss: 0.4010 - val_accuracy: 0.8268\n","Epoch 114/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4563 - accuracy: 0.8006 - val_loss: 0.4031 - val_accuracy: 0.8212\n","Epoch 115/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4557 - accuracy: 0.7992 - val_loss: 0.3997 - val_accuracy: 0.8324\n","Epoch 116/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4558 - accuracy: 0.7992 - val_loss: 0.4001 - val_accuracy: 0.8324\n","Epoch 117/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4560 - accuracy: 0.7992 - val_loss: 0.4004 - val_accuracy: 0.8212\n","Epoch 118/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4556 - accuracy: 0.7949 - val_loss: 0.4002 - val_accuracy: 0.8324\n","Epoch 119/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4558 - accuracy: 0.7978 - val_loss: 0.4014 - val_accuracy: 0.8268\n","Epoch 120/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4557 - accuracy: 0.7992 - val_loss: 0.4020 - val_accuracy: 0.8268\n","Epoch 121/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4553 - accuracy: 0.7992 - val_loss: 0.4016 - val_accuracy: 0.8212\n","Epoch 122/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4554 - accuracy: 0.8020 - val_loss: 0.4000 - val_accuracy: 0.8212\n","Epoch 123/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4551 - accuracy: 0.7992 - val_loss: 0.3997 - val_accuracy: 0.8324\n","Epoch 124/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4553 - accuracy: 0.7992 - val_loss: 0.3999 - val_accuracy: 0.8268\n","Epoch 125/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4575 - accuracy: 0.8034 - val_loss: 0.4002 - val_accuracy: 0.8212\n","Epoch 126/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4552 - accuracy: 0.8006 - val_loss: 0.3986 - val_accuracy: 0.8268\n","Epoch 127/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4556 - accuracy: 0.7992 - val_loss: 0.3993 - val_accuracy: 0.8324\n","Epoch 128/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4569 - accuracy: 0.8020 - val_loss: 0.4002 - val_accuracy: 0.8268\n","Epoch 129/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4556 - accuracy: 0.7978 - val_loss: 0.3994 - val_accuracy: 0.8268\n","Epoch 130/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4564 - accuracy: 0.7992 - val_loss: 0.3986 - val_accuracy: 0.8324\n","Epoch 131/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4558 - accuracy: 0.8020 - val_loss: 0.3991 - val_accuracy: 0.8268\n","Epoch 132/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4554 - accuracy: 0.7978 - val_loss: 0.3977 - val_accuracy: 0.8268\n","Epoch 133/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4553 - accuracy: 0.7992 - val_loss: 0.3986 - val_accuracy: 0.8268\n","Epoch 134/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4549 - accuracy: 0.8006 - val_loss: 0.3994 - val_accuracy: 0.8268\n","Epoch 135/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4551 - accuracy: 0.8020 - val_loss: 0.3985 - val_accuracy: 0.8268\n","Epoch 136/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4553 - accuracy: 0.8006 - val_loss: 0.3987 - val_accuracy: 0.8268\n","Epoch 137/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4552 - accuracy: 0.8006 - val_loss: 0.3973 - val_accuracy: 0.8324\n","Epoch 138/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4556 - accuracy: 0.7992 - val_loss: 0.3977 - val_accuracy: 0.8268\n","Epoch 139/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4553 - accuracy: 0.7992 - val_loss: 0.3984 - val_accuracy: 0.8268\n","Epoch 140/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4578 - accuracy: 0.7963 - val_loss: 0.3978 - val_accuracy: 0.8324\n","Epoch 141/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4550 - accuracy: 0.7992 - val_loss: 0.3964 - val_accuracy: 0.8324\n","Epoch 142/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4547 - accuracy: 0.7949 - val_loss: 0.3978 - val_accuracy: 0.8212\n","Epoch 143/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4548 - accuracy: 0.8006 - val_loss: 0.3987 - val_accuracy: 0.8156\n","Epoch 144/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4553 - accuracy: 0.7992 - val_loss: 0.3971 - val_accuracy: 0.8268\n","Epoch 145/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4547 - accuracy: 0.8006 - val_loss: 0.3959 - val_accuracy: 0.8268\n","Epoch 146/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4546 - accuracy: 0.7992 - val_loss: 0.3975 - val_accuracy: 0.8212\n","Epoch 147/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4558 - accuracy: 0.8006 - val_loss: 0.3979 - val_accuracy: 0.8268\n","Epoch 148/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4545 - accuracy: 0.8020 - val_loss: 0.3978 - val_accuracy: 0.8212\n","Epoch 149/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4547 - accuracy: 0.8006 - val_loss: 0.3970 - val_accuracy: 0.8212\n","Epoch 150/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4551 - accuracy: 0.7992 - val_loss: 0.3964 - val_accuracy: 0.8268\n","Epoch 151/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4544 - accuracy: 0.8006 - val_loss: 0.3959 - val_accuracy: 0.8268\n","Epoch 152/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4550 - accuracy: 0.8006 - val_loss: 0.3964 - val_accuracy: 0.8268\n","Epoch 153/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4554 - accuracy: 0.7963 - val_loss: 0.3960 - val_accuracy: 0.8268\n","Epoch 154/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4555 - accuracy: 0.7992 - val_loss: 0.3969 - val_accuracy: 0.8212\n","Epoch 155/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4555 - accuracy: 0.8006 - val_loss: 0.3964 - val_accuracy: 0.8268\n","Epoch 156/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4546 - accuracy: 0.8034 - val_loss: 0.3956 - val_accuracy: 0.8268\n","Epoch 157/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4551 - accuracy: 0.7978 - val_loss: 0.3961 - val_accuracy: 0.8212\n","Epoch 158/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4563 - accuracy: 0.8034 - val_loss: 0.3955 - val_accuracy: 0.8268\n","Epoch 159/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4548 - accuracy: 0.8020 - val_loss: 0.3977 - val_accuracy: 0.8212\n","Epoch 160/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4548 - accuracy: 0.8020 - val_loss: 0.3957 - val_accuracy: 0.8212\n","Epoch 161/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4551 - accuracy: 0.8006 - val_loss: 0.3968 - val_accuracy: 0.8212\n","Epoch 162/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4543 - accuracy: 0.8006 - val_loss: 0.3950 - val_accuracy: 0.8268\n","Epoch 163/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4544 - accuracy: 0.8006 - val_loss: 0.3969 - val_accuracy: 0.8212\n","Epoch 164/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4555 - accuracy: 0.8006 - val_loss: 0.3971 - val_accuracy: 0.8212\n","Epoch 165/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4558 - accuracy: 0.8020 - val_loss: 0.3958 - val_accuracy: 0.8212\n","Epoch 166/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4556 - accuracy: 0.8006 - val_loss: 0.3962 - val_accuracy: 0.8212\n","Epoch 167/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4542 - accuracy: 0.7992 - val_loss: 0.3937 - val_accuracy: 0.8212\n","Epoch 168/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4541 - accuracy: 0.8006 - val_loss: 0.3954 - val_accuracy: 0.8268\n","Epoch 169/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4556 - accuracy: 0.7978 - val_loss: 0.3951 - val_accuracy: 0.8268\n","Epoch 170/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4573 - accuracy: 0.7978 - val_loss: 0.3947 - val_accuracy: 0.8268\n","Epoch 171/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4551 - accuracy: 0.8006 - val_loss: 0.3951 - val_accuracy: 0.8268\n","Epoch 172/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4548 - accuracy: 0.8020 - val_loss: 0.3951 - val_accuracy: 0.8268\n","Epoch 173/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4557 - accuracy: 0.7921 - val_loss: 0.3967 - val_accuracy: 0.8212\n","Epoch 174/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4560 - accuracy: 0.8006 - val_loss: 0.3942 - val_accuracy: 0.8212\n","Epoch 175/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4562 - accuracy: 0.8006 - val_loss: 0.3947 - val_accuracy: 0.8268\n","Epoch 176/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4541 - accuracy: 0.8006 - val_loss: 0.3932 - val_accuracy: 0.8212\n","Epoch 177/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4545 - accuracy: 0.8006 - val_loss: 0.3937 - val_accuracy: 0.8212\n","Epoch 178/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4545 - accuracy: 0.8006 - val_loss: 0.3940 - val_accuracy: 0.8212\n","Epoch 179/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4557 - accuracy: 0.7963 - val_loss: 0.3944 - val_accuracy: 0.8212\n","Epoch 180/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4553 - accuracy: 0.8006 - val_loss: 0.3926 - val_accuracy: 0.8212\n","Epoch 181/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4552 - accuracy: 0.8034 - val_loss: 0.3944 - val_accuracy: 0.8268\n","Epoch 182/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4546 - accuracy: 0.7992 - val_loss: 0.3955 - val_accuracy: 0.8268\n","Epoch 183/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4551 - accuracy: 0.8006 - val_loss: 0.3935 - val_accuracy: 0.8268\n","Epoch 184/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4556 - accuracy: 0.8006 - val_loss: 0.3930 - val_accuracy: 0.8268\n","Epoch 185/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4543 - accuracy: 0.8006 - val_loss: 0.3951 - val_accuracy: 0.8212\n","Epoch 186/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4542 - accuracy: 0.8034 - val_loss: 0.3927 - val_accuracy: 0.8212\n","Epoch 187/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4541 - accuracy: 0.8006 - val_loss: 0.3931 - val_accuracy: 0.8268\n","Epoch 188/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4548 - accuracy: 0.7992 - val_loss: 0.3927 - val_accuracy: 0.8212\n","Epoch 189/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4544 - accuracy: 0.7992 - val_loss: 0.3922 - val_accuracy: 0.8268\n","Epoch 190/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4542 - accuracy: 0.8006 - val_loss: 0.3922 - val_accuracy: 0.8268\n","Epoch 191/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4550 - accuracy: 0.8006 - val_loss: 0.3918 - val_accuracy: 0.8212\n","Epoch 192/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4551 - accuracy: 0.7935 - val_loss: 0.3946 - val_accuracy: 0.8268\n","Epoch 193/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4566 - accuracy: 0.8020 - val_loss: 0.3914 - val_accuracy: 0.8212\n","Epoch 194/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4549 - accuracy: 0.8034 - val_loss: 0.3955 - val_accuracy: 0.8212\n","Epoch 195/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4550 - accuracy: 0.7935 - val_loss: 0.3932 - val_accuracy: 0.8212\n","Epoch 196/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4565 - accuracy: 0.8048 - val_loss: 0.3949 - val_accuracy: 0.8212\n","Epoch 197/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4545 - accuracy: 0.7992 - val_loss: 0.3949 - val_accuracy: 0.8268\n","Epoch 198/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4552 - accuracy: 0.7992 - val_loss: 0.3939 - val_accuracy: 0.8212\n","Epoch 199/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4545 - accuracy: 0.8006 - val_loss: 0.3933 - val_accuracy: 0.8212\n","Epoch 200/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4548 - accuracy: 0.7992 - val_loss: 0.3930 - val_accuracy: 0.8212\n","Epoch 201/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4558 - accuracy: 0.8006 - val_loss: 0.3923 - val_accuracy: 0.8268\n","Epoch 202/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4559 - accuracy: 0.8006 - val_loss: 0.3924 - val_accuracy: 0.8212\n","Epoch 203/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4548 - accuracy: 0.8006 - val_loss: 0.3941 - val_accuracy: 0.8212\n","Epoch 204/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4540 - accuracy: 0.8006 - val_loss: 0.3924 - val_accuracy: 0.8212\n","Epoch 205/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4568 - accuracy: 0.7978 - val_loss: 0.3923 - val_accuracy: 0.8212\n","Epoch 206/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4539 - accuracy: 0.8006 - val_loss: 0.3937 - val_accuracy: 0.8268\n","Epoch 207/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4550 - accuracy: 0.8006 - val_loss: 0.3906 - val_accuracy: 0.8212\n","Epoch 208/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4566 - accuracy: 0.8006 - val_loss: 0.3927 - val_accuracy: 0.8324\n","Epoch 209/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4568 - accuracy: 0.7949 - val_loss: 0.3909 - val_accuracy: 0.8212\n","Epoch 210/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4539 - accuracy: 0.7992 - val_loss: 0.3930 - val_accuracy: 0.8268\n","Epoch 211/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4551 - accuracy: 0.7978 - val_loss: 0.3932 - val_accuracy: 0.8212\n","Epoch 212/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4547 - accuracy: 0.8006 - val_loss: 0.3910 - val_accuracy: 0.8212\n","Epoch 213/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4551 - accuracy: 0.7992 - val_loss: 0.3926 - val_accuracy: 0.8212\n","Epoch 214/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4548 - accuracy: 0.8006 - val_loss: 0.3933 - val_accuracy: 0.8268\n","Epoch 215/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4552 - accuracy: 0.7992 - val_loss: 0.3932 - val_accuracy: 0.8212\n","Epoch 216/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4546 - accuracy: 0.8020 - val_loss: 0.3916 - val_accuracy: 0.8212\n","Epoch 217/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4540 - accuracy: 0.7992 - val_loss: 0.3930 - val_accuracy: 0.8212\n","Epoch 218/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4562 - accuracy: 0.7963 - val_loss: 0.3931 - val_accuracy: 0.8212\n","Epoch 219/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4583 - accuracy: 0.8006 - val_loss: 0.3910 - val_accuracy: 0.8212\n","Epoch 220/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4565 - accuracy: 0.7963 - val_loss: 0.3935 - val_accuracy: 0.8268\n","Epoch 221/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4553 - accuracy: 0.8006 - val_loss: 0.3926 - val_accuracy: 0.8212\n","Epoch 222/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4537 - accuracy: 0.8006 - val_loss: 0.3936 - val_accuracy: 0.8212\n","Epoch 223/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4560 - accuracy: 0.7963 - val_loss: 0.3938 - val_accuracy: 0.8212\n","Epoch 224/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4554 - accuracy: 0.7978 - val_loss: 0.3917 - val_accuracy: 0.8212\n","Epoch 225/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4546 - accuracy: 0.8006 - val_loss: 0.3914 - val_accuracy: 0.8212\n","Epoch 226/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4558 - accuracy: 0.8020 - val_loss: 0.3923 - val_accuracy: 0.8212\n","Epoch 227/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4540 - accuracy: 0.8006 - val_loss: 0.3926 - val_accuracy: 0.8212\n","Epoch 228/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4546 - accuracy: 0.8006 - val_loss: 0.3930 - val_accuracy: 0.8212\n","Epoch 229/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4555 - accuracy: 0.8020 - val_loss: 0.3938 - val_accuracy: 0.8268\n","Epoch 230/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4536 - accuracy: 0.8006 - val_loss: 0.3911 - val_accuracy: 0.8212\n","Epoch 231/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4555 - accuracy: 0.8020 - val_loss: 0.3904 - val_accuracy: 0.8212\n","Epoch 232/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4535 - accuracy: 0.8006 - val_loss: 0.3928 - val_accuracy: 0.8268\n","Epoch 233/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4545 - accuracy: 0.7992 - val_loss: 0.3934 - val_accuracy: 0.8212\n","Epoch 234/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4539 - accuracy: 0.8006 - val_loss: 0.3910 - val_accuracy: 0.8212\n","Epoch 235/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4551 - accuracy: 0.8020 - val_loss: 0.3921 - val_accuracy: 0.8212\n","Epoch 236/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4570 - accuracy: 0.7978 - val_loss: 0.3917 - val_accuracy: 0.8212\n","Epoch 237/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4557 - accuracy: 0.7935 - val_loss: 0.3918 - val_accuracy: 0.8212\n","Epoch 238/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4537 - accuracy: 0.7978 - val_loss: 0.3924 - val_accuracy: 0.8324\n","Epoch 239/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4541 - accuracy: 0.8006 - val_loss: 0.3921 - val_accuracy: 0.8212\n","Epoch 240/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4552 - accuracy: 0.8006 - val_loss: 0.3916 - val_accuracy: 0.8212\n","Epoch 241/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4556 - accuracy: 0.7992 - val_loss: 0.3932 - val_accuracy: 0.8212\n","Epoch 242/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4544 - accuracy: 0.8006 - val_loss: 0.3921 - val_accuracy: 0.8212\n","Epoch 243/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4542 - accuracy: 0.8006 - val_loss: 0.3906 - val_accuracy: 0.8268\n","Epoch 244/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4552 - accuracy: 0.7907 - val_loss: 0.3924 - val_accuracy: 0.8268\n","Epoch 245/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4540 - accuracy: 0.8006 - val_loss: 0.3896 - val_accuracy: 0.8212\n","Epoch 246/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4567 - accuracy: 0.8020 - val_loss: 0.3889 - val_accuracy: 0.8212\n","Epoch 247/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4536 - accuracy: 0.7963 - val_loss: 0.3933 - val_accuracy: 0.8268\n","Epoch 248/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4546 - accuracy: 0.8020 - val_loss: 0.3900 - val_accuracy: 0.8212\n","Epoch 249/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4549 - accuracy: 0.8006 - val_loss: 0.3917 - val_accuracy: 0.8268\n","Epoch 250/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4551 - accuracy: 0.8006 - val_loss: 0.3900 - val_accuracy: 0.8212\n","Epoch 251/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4562 - accuracy: 0.7949 - val_loss: 0.3934 - val_accuracy: 0.8268\n","Epoch 252/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4553 - accuracy: 0.7978 - val_loss: 0.3902 - val_accuracy: 0.8212\n","Epoch 253/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4540 - accuracy: 0.8006 - val_loss: 0.3912 - val_accuracy: 0.8212\n","Epoch 254/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4544 - accuracy: 0.8006 - val_loss: 0.3902 - val_accuracy: 0.8212\n","Epoch 255/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4544 - accuracy: 0.7992 - val_loss: 0.3923 - val_accuracy: 0.8268\n","Epoch 256/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4540 - accuracy: 0.8006 - val_loss: 0.3896 - val_accuracy: 0.8212\n","Epoch 257/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4537 - accuracy: 0.8006 - val_loss: 0.3911 - val_accuracy: 0.8212\n","Epoch 258/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4541 - accuracy: 0.8006 - val_loss: 0.3915 - val_accuracy: 0.8212\n","Epoch 259/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4552 - accuracy: 0.8006 - val_loss: 0.3916 - val_accuracy: 0.8268\n","Epoch 260/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4541 - accuracy: 0.8006 - val_loss: 0.3893 - val_accuracy: 0.8212\n","Epoch 261/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4544 - accuracy: 0.8006 - val_loss: 0.3916 - val_accuracy: 0.8212\n","Epoch 262/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4548 - accuracy: 0.8006 - val_loss: 0.3912 - val_accuracy: 0.8212\n","Epoch 263/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4552 - accuracy: 0.7837 - val_loss: 0.3911 - val_accuracy: 0.8212\n","Epoch 264/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4541 - accuracy: 0.8006 - val_loss: 0.3908 - val_accuracy: 0.8212\n","Epoch 265/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4544 - accuracy: 0.8006 - val_loss: 0.3908 - val_accuracy: 0.8212\n","Epoch 266/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4546 - accuracy: 0.7992 - val_loss: 0.3912 - val_accuracy: 0.8212\n","Epoch 267/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4559 - accuracy: 0.7978 - val_loss: 0.3906 - val_accuracy: 0.8212\n","Epoch 268/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4550 - accuracy: 0.8006 - val_loss: 0.3903 - val_accuracy: 0.8212\n","Epoch 269/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4553 - accuracy: 0.7963 - val_loss: 0.3907 - val_accuracy: 0.8212\n","Epoch 270/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4543 - accuracy: 0.7963 - val_loss: 0.3908 - val_accuracy: 0.8212\n","Epoch 271/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4548 - accuracy: 0.8006 - val_loss: 0.3905 - val_accuracy: 0.8212\n","Epoch 272/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4538 - accuracy: 0.8006 - val_loss: 0.3908 - val_accuracy: 0.8212\n","Epoch 273/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4546 - accuracy: 0.7992 - val_loss: 0.3925 - val_accuracy: 0.8268\n","Epoch 274/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4543 - accuracy: 0.8006 - val_loss: 0.3909 - val_accuracy: 0.8212\n","Epoch 275/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4544 - accuracy: 0.8006 - val_loss: 0.3917 - val_accuracy: 0.8268\n","Epoch 276/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4558 - accuracy: 0.7963 - val_loss: 0.3906 - val_accuracy: 0.8212\n","Epoch 277/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4543 - accuracy: 0.7978 - val_loss: 0.3901 - val_accuracy: 0.8324\n","Epoch 278/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4537 - accuracy: 0.8006 - val_loss: 0.3913 - val_accuracy: 0.8212\n","Epoch 279/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4542 - accuracy: 0.7992 - val_loss: 0.3934 - val_accuracy: 0.8212\n","Epoch 280/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4548 - accuracy: 0.7992 - val_loss: 0.3919 - val_accuracy: 0.8212\n","Epoch 281/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4547 - accuracy: 0.7992 - val_loss: 0.3901 - val_accuracy: 0.8268\n","Epoch 282/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4542 - accuracy: 0.8006 - val_loss: 0.3905 - val_accuracy: 0.8212\n","Epoch 283/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4542 - accuracy: 0.8006 - val_loss: 0.3926 - val_accuracy: 0.8212\n","Epoch 284/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4552 - accuracy: 0.7978 - val_loss: 0.3897 - val_accuracy: 0.8268\n","Epoch 285/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4544 - accuracy: 0.8006 - val_loss: 0.3900 - val_accuracy: 0.8268\n","Epoch 286/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4544 - accuracy: 0.7992 - val_loss: 0.3916 - val_accuracy: 0.8268\n","Epoch 287/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4542 - accuracy: 0.8006 - val_loss: 0.3894 - val_accuracy: 0.8268\n","Epoch 288/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4540 - accuracy: 0.8006 - val_loss: 0.3907 - val_accuracy: 0.8268\n","Epoch 289/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4541 - accuracy: 0.8020 - val_loss: 0.3893 - val_accuracy: 0.8268\n","Epoch 290/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4543 - accuracy: 0.7992 - val_loss: 0.3913 - val_accuracy: 0.8268\n","Epoch 291/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4536 - accuracy: 0.8006 - val_loss: 0.3901 - val_accuracy: 0.8268\n","Epoch 292/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4541 - accuracy: 0.8006 - val_loss: 0.3899 - val_accuracy: 0.8268\n","Epoch 293/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4547 - accuracy: 0.7992 - val_loss: 0.3901 - val_accuracy: 0.8268\n","Epoch 294/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4554 - accuracy: 0.7978 - val_loss: 0.3900 - val_accuracy: 0.8212\n","Epoch 295/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4562 - accuracy: 0.8048 - val_loss: 0.3899 - val_accuracy: 0.8268\n","Epoch 296/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4552 - accuracy: 0.7978 - val_loss: 0.3898 - val_accuracy: 0.8268\n","Epoch 297/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4551 - accuracy: 0.7978 - val_loss: 0.3902 - val_accuracy: 0.8268\n","Epoch 298/300\n","23/23 [==============================] - 0s 3ms/step - loss: 0.4546 - accuracy: 0.8006 - val_loss: 0.3899 - val_accuracy: 0.8212\n","Epoch 299/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4552 - accuracy: 0.7935 - val_loss: 0.3903 - val_accuracy: 0.8212\n","Epoch 300/300\n","23/23 [==============================] - 0s 4ms/step - loss: 0.4545 - accuracy: 0.7949 - val_loss: 0.3913 - val_accuracy: 0.8212\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7f63ac6afdf0>"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["# 만들어진 모델을 이용해서 제출파일을 만들어야 해요!\n","# 제공된 test.csv를 이용해서 파일을 만들면 되요!\n","\n","# test.csv를 이용해서 우리 모델에 대한 예측값을 얻고\n","# 그 값으로 gender_submission.csv 파일 형식으로 결과 파일을 만들어서\n","# Kaggle에 upload해서 모델의 정확도를 측정 받으시면 되요!\n"],"metadata":{"id":"CItieu5AIT3Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# scikit-learn으로 학습하고 평가까지 진행해 보아요!\n","\n","# scikit-learn으로 학습하고 평가까지 진행하려면 당연히\n","# test data가 있어야 해요!\n","\n","# 전체 데이터를 training data와 test data로 분리할꺼예요!\n","# 여기서 말하는 전체데이터는 x_data, t_data를 의미해요\n","# 7:3 비율로 데이터를 나눌꺼예요!\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn import linear_model\n","\n","x_data_train, x_data_test, t_data_train, t_data_test = \\\n","train_test_split(x_data,\n","                 t_data,\n","                 test_size=0.3)\n","\n","sklearn_model = linear_model.LogisticRegression()\n","sklearn_model.fit(x_data_train, t_data_train.ravel())\n","\n","sklearn_model.score(x_data_test, t_data_test)  # 0.8171641791044776"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GDR9BrKTBgyw","executionInfo":{"status":"ok","timestamp":1698384595195,"user_tz":-540,"elapsed":395,"user":{"displayName":"문성훈","userId":"17340983977521415755"}},"outputId":"ebfcdad8-534e-44cc-def3-76bdaa9b9a45"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7910447761194029"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["test = pd.read_csv('/content/test.csv')\n","submission = pd.read_csv('/content/gender_submission.csv')\n","\n","# display(test.head())\n","# display(submission.head())\n","\n","# 테스트데이터 전처리\n","# 사용하지 않는 column 제거\n","test.drop(['PassengerId', 'Name', 'Ticket', 'Fare', 'Cabin'], axis=1, inplace=True)\n","\n","# 성별 처리\n","gender_mapping = { 'male' : 0, 'female' : 1 }\n","test['Sex'] = test['Sex'].map(gender_mapping)\n","\n","# 가족처리\n","test['Family'] = test['SibSp'] + test['Parch']\n","test.drop(['SibSp', 'Parch'], axis=1, inplace=True)\n","\n","# Embarked 결측치 처리\n","test['Embarked'] = test['Embarked'].fillna('Q')\n","\n","# Embarked 문자를 숫자로 변환\n","embarked_mapping = { 'S' : 0, 'C' : 1, 'Q' : 2 }\n","test['Embarked'] = test['Embarked'].map(embarked_mapping)\n","\n","# Age 결측치 처리\n","test['Age'] = test['Age'].fillna(test['Age'].mean())\n","\n","# Age Binning 처리(Numerical Value -> Categorical Value)\n","test.loc[test['Age'] < 8, 'Age'] = 0\n","test.loc[(test['Age'] >= 8) & (test['Age'] < 20), 'Age'] = 1\n","test.loc[(test['Age'] >= 20) & (test['Age'] < 65), 'Age'] = 2\n","test.loc[test['Age'] >= 65, 'Age'] = 3\n","\n","# display(test)\n","\n","predict = keras_model.predict(test)\n","\n","submission['Survived'] = predict\n","submission['Survived'] = (submission['Survived'] > 0.5).astype('int')\n","submission.to_csv('sub.csv', index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VDp9fBkFmWMJ","executionInfo":{"status":"ok","timestamp":1698385276224,"user_tz":-540,"elapsed":588,"user":{"displayName":"문성훈","userId":"17340983977521415755"}},"outputId":"ac32e207-0a5b-4524-a879-276b1eb1c4c8"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["14/14 [==============================] - 0s 2ms/step\n"]}]},{"cell_type":"code","source":["# Multinomial Classification을 구현해 보아요!\n","# 사용하는 데이터셋은 BMI 데이터셋이예요!\n","\n","# tensorflow keras와 scikit-learn을 이용해서 구현해보아요!\n","\n","# 일단 필요한 모듈부터 import를 해 보아요!\n","import numpy as np\n","import pandas as pd\n","from sklearn import linear_model\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","from scipy import stats\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Flatten, Dense\n","from tensorflow.keras.optimizers import Adam\n","import matplotlib.pyplot as plt\n","\n","# Raw Data Loading\n","df = pd.read_csv('/content/drive/MyDrive/[빅데이터 과정 공유폴더]/data/bmi/bmi.csv',\n","                 skiprows=3)\n","# display(df.head(), df.shape)  # (20000, 3)\n","\n","# 데이터 전처리\n","# 1. 결측치를 찾아서 만약 존재하면 처리해야 해요!\n","# df.info()  # 결측치가 없네요!\n","# df.isnull().sum()\n","\n","# 2. 이상치를 처리해야 해요! => 없어요!!\n","# plt.boxplot(df['label'].values)  # label에는 이상치가 없어요!\n","# plt.hist(df['label'].values, bins=3)\n","# plt.boxplot(df['height'].values)  # height에는 이상치가 없어요!\n","# plt.boxplot(df['weight'].values)  # weight에는 이상치가 없어요!\n","# plt.show()\n","\n","# 3. 정규화를 해야 해요!\n","x_data = df[['height', 'weight']].values  # 2차원\n","t_data = df['label']   # 1차원\n","scaler = MinMaxScaler()\n","scaler.fit(x_data)\n","\n","x_data_norm = scaler.transform(x_data)\n","\n","# 4. train data와 test data를 분할해야 해요~!\n","x_data_train, x_data_test, t_data_train, t_data_test = \\\n","train_test_split(x_data,\n","                 t_data,\n","                 test_size=0.3)\n",""],"metadata":{"id":"vTV6BQt19WV8","executionInfo":{"status":"ok","timestamp":1698392206911,"user_tz":-540,"elapsed":10,"user":{"displayName":"문성훈","userId":"17340983977521415755"}}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":["# Scikit Learn 구현\n","sklearn_model = linear_model.LogisticRegression()\n","sklearn_model.fit(x_data_train,\n","                  t_data_train)\n","\n","sklearn_model_result = sklearn_model.score(x_data_test,\n","                                           t_data_test)\n","print('sklearn model의 accuracy : {}'.format(sklearn_model_result))\n","# 0.9823\n","# 187, 82\n","print(sklearn_model.predict_proba([[187, 82]]))\n","# [[4.55458751e-17 9.99979818e-01 2.01819517e-05]]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vChqzg95EHcW","executionInfo":{"status":"ok","timestamp":1698392463048,"user_tz":-540,"elapsed":547,"user":{"displayName":"문성훈","userId":"17340983977521415755"}},"outputId":"8ec6aac6-2715-431f-9d8d-17a756ba1013"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["sklearn model의 accuracy : 0.9823333333333333\n","[[4.55458751e-17 9.99979818e-01 2.01819517e-05]]\n"]}]},{"cell_type":"code","source":["# Tensorflow Keras 구현\n","keras_model = Sequential()\n","\n","keras_model.add(Flatten(input_shape=(2,)))\n","keras_model.add(Dense(units=3,\n","                      activation='softmax'))\n","keras_model.compile(optimizer=Adam(learning_rate=1e-2),\n","                    loss='sparse_categorical_crossentropy',\n","                    metrics=['accuracy'])\n","\n","# 정규화된 데이터를 사용해야 해요!\n","x_data_norm_train, x_data_norm_test, t_data_train, t_data_test = \\\n","train_test_split(x_data_norm,\n","                 t_data,\n","                 test_size=0.3)\n","keras_model.fit(x_data_norm_train,\n","                t_data_train,\n","                epochs=200,\n","                verbose=1,\n","                validation_split=0.3)"],"metadata":{"id":"ubE532vMFdIt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 학습이 다 되면!!\n","# 평가를 해야 해요!\n","print(keras_model.evaluate(x_data_norm_test, t_data_test))\n","# [0.060929279774427414, 0.9858333468437195]\n","\n","# Prediction\n","my_state = np.array([[187, 82]])\n","my_state_norm = scaler.transform(my_state)\n","print(keras_model.predict(my_state_norm))\n","# [[7.1084266e-08 9.3678677e-01 6.3213095e-02]]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UTCeS_GQLpyS","executionInfo":{"status":"ok","timestamp":1698394427258,"user_tz":-540,"elapsed":595,"user":{"displayName":"문성훈","userId":"17340983977521415755"}},"outputId":"1a073ab0-3b14-4243-cb49-c9049b9c1ca6"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["188/188 [==============================] - 0s 2ms/step - loss: 0.0609 - accuracy: 0.9858\n","[0.060929279774427414, 0.9858333468437195]\n","1/1 [==============================] - 0s 36ms/step\n","[[7.1084266e-08 9.3678677e-01 6.3213095e-02]]\n"]}]}]}